Will Dolan
wrdolan@usc.edu

Using ResNet50, I categorized images of the ASL alphabet at a >99% training accuracy rate with a loss as low as .0834 and a testing accuracy of 98%.

I used a dataset of images of people performing characters in the ASL alphabet from [this kaggle link](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). Because the test folder only had 28 images, I eventually added more images to the test folder. Before I did this, on my first full run of 10 epochs, the original testing images were graded with 100% accuracy. After adding in 10% of the training images to testing, the final testing accuracy came out to be 98%.

I implemented ResNet50 for this project. I chose ResNet50 due to its great performance, and I wanted a faster training time compared to ResNet101. I removed most of the transformations that were listed in places like L4. Resizing the image larger would just slow down the training without adding more usable features or generalization. I originally removed the random horizontal flip thinking that sign language would be different based on the orientation of the hand, but because the ASL alphabet can be done with either hand, the horizontal flip is fine. I used ResNetâ€™s suggested normalizations in the data transforms as well. I chose a learning rate of 0.001 so that the model is not too fast to end up with suboptimal weights, but also fast enough so that I can train the model in the limited time I have with Google Colab. For the sake of time and to avoid overfitting, I only ran 10 epochs. By the end of the 10, the accuracy was very close to 100%, and the loss was making less and less progress, so this seemed like a good place to stop.

The metric I monitored was accuracy. Accuracy is a good metric in this scenario because the data is spread evenly across all classes. Therefore, to get a high accuracy, all classes must perform very well, and the model is not favoring one class over another. If the classes were imbalanced, I would not use accuracy and instead probably favor AUC/ROC for distinguishing between classes. Accuracy is also quite easy and quick to compute without taking much space at all. 

The dataset and model architecture fit perfectly together. The dataset was massive enough to not cause overfitting, and the model picked up on necessary features that gave it such a high accuracy and low loss. This could be 

With the model being this successful, this could definitely be expanded to wider uses and uses for social good. This could be transferred from just the ASL alphabet to ASL in general, helping transcribe sign language for those who are deaf and/or hard of hearing. People that do not understand sign language could use something like this to start to learn sign language, giving them translations they understand. Limitations in the methods could include that these are (for the most part) clear, static images. I am not sure if transferring this to use on videos will be successful if the images are not always clear.

If I continued this project (and had more time and computing power), I would love to play around with a number of hyperparameters and pretrained models to see how they compare against each other. I would also like to try to translate this into video, and make something that essentially acts as a transcriber for ASL on the screen. 
